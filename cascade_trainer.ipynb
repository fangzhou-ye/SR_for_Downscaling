{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b571b7b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b571b7b7",
    "outputId": "1ea477e7-7809-421a-d04a-ecd4c1402cfc"
   },
   "outputs": [],
   "source": [
    "# srcnn sst cascade\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KUSwrPt0NoRt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUSwrPt0NoRt",
    "outputId": "2bad8e07-65b2-4d23-b7f2-a9d4c19ca11b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b990939",
   "metadata": {
    "id": "5b990939"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn.modules.activation import ReLU, Sigmoid\n",
    "from torch.nn import Conv2d, modules\n",
    "from torch.nn import Sequential\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8072b60d",
   "metadata": {
    "id": "8072b60d"
   },
   "outputs": [],
   "source": [
    "# utils functions\n",
    "def img_read(fPath):\n",
    "    '''\n",
    "    read the image given path \"fPath\"\n",
    "    '''\n",
    "    img = cv2.imread(fPath, -1) # single channel image\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def downsample(orig_img, scale):\n",
    "    '''\n",
    "    downsample by \"scale\" to get the low resolution image\n",
    "    '''\n",
    "    if scale == 1:\n",
    "        return orig_img\n",
    "    h_orig, w_orig = orig_img.shape\n",
    "    h, w = int(h_orig/scale), int(w_orig/scale)\n",
    "    return cv2.resize(orig_img, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def bicubic_sr(lr_img, scale):\n",
    "    '''\n",
    "    bibubic super-resolved reconstruction from lr_img by factor \"scale\"\n",
    "    '''\n",
    "    h, w = lr_img.shape\n",
    "    h_orig, w_orig = h*scale, w*scale\n",
    "    return cv2.resize(lr_img, (w_orig, h_orig), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def computePSNR(img1, img2):\n",
    "    '''\n",
    "    compute PSNR(Peak Signal to Noise Ratio) to calculate accuracy\n",
    "    img1 and img2 have range [0, 1], and both are gray level images\n",
    "    '''\n",
    "    if not img1.shape == img2.shape:\n",
    "        print(\"Input images must have the same dimensions.\")\n",
    "    mse = torch.mean((img1-img2)**2)\n",
    "    if mse == 0: # img1 and img2 are same images\n",
    "        return float('inf')\n",
    "    return 10.0 * torch.log10(1.0/mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d78590b",
   "metadata": {
    "id": "2d78590b"
   },
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, \n",
    "                               out_channels=64, \n",
    "                               kernel_size=9, \n",
    "                               padding=9 // 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, \n",
    "                               out_channels=32, \n",
    "                               kernel_size=5, \n",
    "                               padding=5 // 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, \n",
    "                               out_channels=num_channels, \n",
    "                               kernel_size=5, \n",
    "                               padding=5 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8ef73",
   "metadata": {
    "id": "30b8ef73"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770fc52",
   "metadata": {
    "id": "5770fc52"
   },
   "outputs": [],
   "source": [
    "class SRData(Dataset):\n",
    "    def __init__(self, dataRoot=\"../dataset/\", field=\"sst\", inter_scale=3, lr_scale=9, transform=None):\n",
    "        self.dataRoot = dataRoot\n",
    "        self.field = field\n",
    "        self.inter_scale = inter_scale\n",
    "        self.lr_scale = lr_scale\n",
    "        self.transform = transform\n",
    "        self.patches = self.getPatches()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        T1 = img_read(self.patches[index])# 90*90, GT for second stage\n",
    "        T3 = downsample(T1, self.inter_scale) #30*30, GT of first stage\n",
    "        T9 = downsample(T1, self.lr_scale) # 10*10\n",
    "        bicubT9 = bicubic_sr(T9, scale=int(self.lr_scale/self.inter_scale)) # 30*30 input of first stage \n",
    "        if self.transform:\n",
    "            T1 = self.transform(T1)\n",
    "            T3 = self.transform(T3)\n",
    "            T9 = self.transform(T9)\n",
    "            bicubT9 = self.transform(bicubT9)\n",
    "        return T1, T3, T9, bicubT9\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def getPatches(self):\n",
    "        '''\n",
    "        get the list of patches sorted by order\n",
    "        '''\n",
    "        dataset = os.path.join(self.dataRoot, self.field)\n",
    "        patches = []\n",
    "        for date in os.listdir(dataset):\n",
    "            dateFolder = os.path.join(dataset, date)\n",
    "            for patch in os.listdir(dateFolder):\n",
    "                patches.append(os.path.join(dateFolder, patch))\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e3156",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e1e3156",
    "outputId": "900ae1bd-d10e-4f3a-b546-43da3f22ca91"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "hyper parameters\n",
    "'''\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    print(\"cuda available\")\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "LR = 1e-4\n",
    "EPOCHS = 200\n",
    "verbose = 1\n",
    "ALPHA = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c685050e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c685050e",
    "outputId": "f65bca75-f60b-4f21-e73b-06ca08231d5f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "prepare data\n",
    "'''\n",
    "# convert input data to normalized tensor\n",
    "trans_input = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0), (1.0)) # do nothing\n",
    "])\n",
    "\n",
    "trans_img = transforms.ToPILImage()\n",
    "\n",
    "trans_bicub = transforms.Resize(size=90, interpolation=InterpolationMode.BICUBIC)\n",
    "\n",
    "data = SRData(dataRoot=\"dataset\", field=\"sst\", inter_scale=3, lr_scale=9, transform=trans_input)\n",
    "\n",
    "train_indices = torch.arange(44800)\n",
    "val_indices = torch.arange(44800, 57600)\n",
    "\n",
    "train_data = torch.utils.data.Subset(data, train_indices)\n",
    "val_data = torch.utils.data.Subset(data, val_indices)\n",
    "# data size 7 : 2 : 1\n",
    "print(\"train set length: {}\".format(int(len(train_data))))\n",
    "print(\"val set length: {}\".format(int(len(val_data))))\n",
    "# load data\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              pin_memory=True,\n",
    "                              shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_data,\n",
    "                            batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf73f1",
   "metadata": {
    "id": "2cdf73f1"
   },
   "outputs": [],
   "source": [
    "total_train_step = 0    # total training step\n",
    "total_val_step = 0      # total validation step\n",
    "\n",
    "# build model\n",
    "model1 = SRCNN(num_channels=1).to(DEVICE) # first stage model 10->30\n",
    "model2 = SRCNN(num_channels=1).to(DEVICE) # second stage model 30->90\n",
    "# loss function\n",
    "Loss1 = nn.MSELoss() # first stage loss\n",
    "Loss2 = nn.MSELoss() # second stage loss\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model1.conv1.parameters()},\n",
    "    {'params': model1.conv2.parameters()},\n",
    "    {'params': model1.conv3.parameters(),'lr': LR * 0.1},\n",
    "    {'params': model2.conv1.parameters()},\n",
    "    {'params': model2.conv2.parameters()},\n",
    "    {'params': model2.conv3.parameters(),'lr': LR * 0.1}], lr=LR)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21247322",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "21247322",
    "outputId": "91fd82c1-9922-482b-e8be-1d3c96da24df"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "training the SRCNN model\n",
    "'''\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# learning rate scheduler\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5, verbose=True)\n",
    "# visualize (tensorboard)\n",
    "folder = \"/content/drive/MyDrive/PRAT/cascade_0.2\"\n",
    "writer = SummaryWriter(os.path.join(folder, \"logs\"))\n",
    "\n",
    "best_weight1 = copy.deepcopy(model1.state_dict()) #first stage best model\n",
    "best_weight2 = copy.deepcopy(model2.state_dict()) # second stage best model\n",
    "best_epoch = 0\n",
    "best_psnr = 0.0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # training\n",
    "    model1.train()\n",
    "    model2.train()\n",
    "    epoch_losses = AverageMeter()\n",
    "    \n",
    "    with tqdm(total=(len(train_data)), ncols=100) as t1:\n",
    "        t1.set_description('epoch train: {}/{}'.format(i+1, EPOCHS))\n",
    "        for data in train_dataloader:\n",
    "            \n",
    "            # get data, feed into device\n",
    "            batch_T1, batch_T3, batch_T9, batch_bicubT9 = data\n",
    "            batch_T1 = batch_T1.to(DEVICE)\n",
    "            batch_T3 = batch_T3.to(DEVICE)\n",
    "            batch_T9 = batch_T9.to(DEVICE)\n",
    "            batch_bicubT9 = batch_bicubT9.to(DEVICE)\n",
    "            \n",
    "            batch_I3 = model1(batch_bicubT9) # 30*30 output of model\n",
    "            batch_bicubI3 = trans_bicub(batch_I3) # 90*90, input of model_prime\n",
    "            batch_I1 = model2(batch_bicubI3)# 90*90, output of model_prime\n",
    "            \n",
    "            # calculate weighted loss\n",
    "            l1 = Loss1(batch_T3, batch_I3)\n",
    "            l2 = Loss2(batch_T1, batch_I1)\n",
    "            loss_total = ALPHA*l2 + (1-ALPHA)*l1\n",
    "            epoch_losses.update(loss_total.item(), len(batch_I1))\n",
    "            \n",
    "            # optimizer\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss\n",
    "            t1.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "\n",
    "            # show on tensorboard\n",
    "            total_train_step += 1            \n",
    "            if total_train_step%1000 == 0:\n",
    "            #    print(\"train step: {}，Loss: {}\".format(total_train_step, loss.item()))\n",
    "                writer.add_scalar(\"train_loss\", epoch_losses.avg, total_train_step)\n",
    "            \n",
    "            # update tqdm\n",
    "            t1.update(len(batch_T1))\n",
    "\n",
    "    # validation\n",
    "    if (i+1) % verbose == 0:\n",
    "        \n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        epoch_psnr = AverageMeter()\n",
    "        cubic_psnr = AverageMeter()\n",
    "\n",
    "        for data in val_dataloader:\n",
    "            \n",
    "             # get data, feed into device\n",
    "            batch_T1, batch_T3, batch_T9, batch_bicubT9 = data\n",
    "            batch_T1 = batch_T1.to(DEVICE)\n",
    "            batch_T3 = batch_T3.to(DEVICE)\n",
    "            batch_T9 = batch_T9.to(DEVICE)\n",
    "            batch_bicubT9 = batch_bicubT9.to(DEVICE)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_I3 = model1(batch_bicubT9) # 30*30 output of model1\n",
    "                batch_bicubI3 = trans_bicub(batch_I3) # 90*90 input of model2\n",
    "                batch_I1 = model2(batch_bicubI3).clamp(0.0, 1.0)# 90*90, output of model2\n",
    "            \n",
    "            # calculate psnr\n",
    "            psnr = computePSNR(batch_T1, batch_I1)\n",
    "            # update total psnr\n",
    "            epoch_psnr.update(psnr, len(batch_T1))\n",
    "        \n",
    "        # print psnr\n",
    "        print('val set PSNR: {:.4f}'.format(epoch_psnr.avg))\n",
    "#         print('cubic PSNR: {:.4f}'.format(cubic_psnr.avg))\n",
    "        \n",
    "        # show in tensorboard\n",
    "        total_val_step += verbose\n",
    "        writer.add_scalar(\"val_psnr\", epoch_psnr.avg, total_val_step) \n",
    "        # save best weights\n",
    "        if epoch_psnr.avg > best_psnr:\n",
    "            best_epoch = i+1\n",
    "            best_psnr = epoch_psnr.avg\n",
    "            best_weight1 = copy.deepcopy(model1.state_dict())\n",
    "            best_weight2 = copy.deepcopy(model2.state_dict())\n",
    "            \n",
    "    # save best models every 100 epochs\n",
    "    if (i+1)%20 == 0:\n",
    "        print('top {} best epoch: {}, val set psnr: {:.4f}'.format(i+1, best_epoch, best_psnr))\n",
    "        torch.save(best_weight1, os.path.join(folder, \"saved_weights/top_{}_best_iter_{}_stage1.pth\".format(i+1, best_epoch)))\n",
    "        torch.save(best_weight2, os.path.join(folder, \"saved_weights/top_{}_best_iter_{}_stage2.pth\".format(i+1, best_epoch)))\n",
    "        print(\"best model in first {} epochs saved\".format(i+1))\n",
    "\n",
    "# close tensorboard\n",
    "writer.close()\n",
    "\n",
    "# save best model\n",
    "print('global best epoch: {}, val set psnr: {:.4f}'.format(best_epoch, best_psnr))\n",
    "torch.save(best_weight1, os.path.join(folder, \"saved_weights/global_best_stage1_iter_{}.pth\".format(best_epoch)))\n",
    "torch.save(best_weight2, os.path.join(folder, \"saved_weights/global_best_stage2_iter_{}.pth\".format(best_epoch)))\n",
    "print(\"global best model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G6yKLK4iGVdU",
   "metadata": {
    "id": "G6yKLK4iGVdU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "cascade_trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
